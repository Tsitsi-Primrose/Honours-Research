{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primrose/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "import scipy\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "import keras as k\n",
    "from keras.layers.core import Layer, Dense\n",
    "import PIL\n",
    "\n",
    "from scipy.io.wavfile import read\n",
    "import pandas as pd\n",
    "import wave\n",
    "import pylab\n",
    "from PIL import Image\n",
    "\n",
    "from scipy.io import wavfile\n",
    "%matplotlib inline\n",
    "\n",
    "import librosa\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "import time\n",
    "\n",
    "sample_length = 44100*2 # 2 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataframes for look-ups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels(path, target_dir):\n",
    "    df_files = pd.DataFrame(columns=['name', 'target'])\n",
    "    count = 0\n",
    "    for audio in os.walk(path):\n",
    "        if audio[0][-6:] == \"female\":\n",
    "            for aud in audio[2]:\n",
    "                df_files.loc[count] = [target_dir+\"/female/\"+aud, 1]\n",
    "                count += 1\n",
    "        elif audio[0][-4:] == \"male\":\n",
    "            for aud in audio[2]:\n",
    "                df_files.loc[count] = [target_dir+\"/male/\"+aud, 0]\n",
    "                count += 1\n",
    "    return df_files      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = labels('/home/primrose/Desktop/Recent/test/', 'test')\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_valid = labels('/home/primrose/Desktop/Recent/valid/', 'valid') \n",
    "df_valid = df_valid.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "df_train = labels('/home/primrose/Desktop/Recent/train/', 'train')\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting mp3 files to wav\n",
    "\n",
    "I used a bash script to convert mp3 files to wav files which is also attached here, convert.sh."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code splits the data into train, test and validation sets. It also extracts the labels from the folder names.\n",
    "\n",
    "The data is in 2 folders, female and male. The female folder has the female audios and the male folder has the male audios. The following functions go through the test, train and validation datasets and put the paths and labels in a dataframe based on the folder the data is in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to split the audio, normalize and draw spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(arr):\n",
    "    if arr.shape[0] >= sample_length:\n",
    "        return arr[:sample_length]\n",
    "    c = np.zeros((sample_length))\n",
    "    c[:arr.shape[0]] = arr\n",
    "    return c\n",
    "\n",
    "def get_spec(arr, framerate):\n",
    "    spec = pylab.specgram(arr, Fs = framerate)\n",
    "    X_scaled = preprocessing.scale(spec[0])\n",
    "    return X_scaled\n",
    "\n",
    "def get_audio(df):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        opened = wave.open(df.at[i, 'name'])\n",
    "        nchannels, sampwidth, framerate, nframes, comptype, compname =  opened.getparams()\n",
    "        dataframes = opened.readframes(-1)\n",
    "        dataframes = np.fromstring(dataframes, 'int32')\n",
    "\n",
    "        c = trim(dataframes)\n",
    "        data.append(c)\n",
    "    return np.array(data)\n",
    "\n",
    "def get_spectrograms(df):\n",
    "    data = []\n",
    "    for i in range(len(df)):\n",
    "        opened = wave.open(df.at[i, 'name'])\n",
    "        nchannels, sampwidth, framerate, nframes, comptype, compname =  opened.getparams()\n",
    "        dataframes = opened.readframes(-1)\n",
    "        dataframes = np.fromstring(dataframes, 'int32')\n",
    "\n",
    "        c = trim(dataframes)\n",
    "        a = get_spec(c, framerate)\n",
    "        data.append(a)\n",
    "    return np.array(data)\n",
    "\n",
    "def normalize_data(data):\n",
    "    max_data = np.max(data)\n",
    "    min_data = np.min(data)\n",
    "    data = (data-min_data)/(max_data-min_data+1e-299)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/primrose/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:19: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(104, 88200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = normalize_data(get_audio(df_train))\n",
    "valid_data = normalize_data(get_audio(df_valid))\n",
    "test_data = normalize_data(get_audio(df_test))\n",
    "\n",
    "# train_data = get_audio(df_train)\n",
    "# valid_data = get_audio(df_valid)\n",
    "# test_data = get_audio(df_test)\n",
    "\n",
    "\n",
    "# train_data = train_data.reshape((train_data.shape[0], train_data.shape[1]*train_data.shape[2]))\n",
    "# valid_data = valid_data.reshape((valid_data.shape[0], valid_data.shape[1]*valid_data.shape[2]))\n",
    "# test_data = test_data.reshape((test_data.shape[0], test_data.shape[1]*test_data.shape[2]))\n",
    "\n",
    "train_labels = df_train['target'].values.astype('int')\n",
    "valid_labels = df_valid['target'].values.astype('int')\n",
    "test_labels = df_test['target'].values.astype('int')\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADIENTS ARE EXPLODING, SEEING THIS THROUGH NAN VALUES AND EXTREMELY LARGE VALUES, try using RELU function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self, visible_number, hidden_number=6, theta=None, hidden_bias=None, visible_bias=None):\n",
    "        self.visible_number = visible_number\n",
    "        self.hidden_number = hidden_number\n",
    "        self.free_energy = []\n",
    "    \n",
    "        self.theta = np.random.random_sample((visible_number, hidden_number)) if theta is None else theta\n",
    "        self.theta = self.array_to_float32_tensor(self.theta)\n",
    "        self.hidden_bias = np.ones((1,hidden_number)) if hidden_bias is None else hidden_bias\n",
    "        self.hidden_bias = self.array_to_float32_tensor(self.hidden_bias)\n",
    "        self.visible_bias = np.ones((1,visible_number)) if visible_bias is None else visible_bias\n",
    "        self.visible_bias = self.array_to_float32_tensor(self.visible_bias)\n",
    "        \n",
    "    def array_to_float32_tensor(self,arr,msg=None):\n",
    "        x=tf.cast(tf.convert_to_tensor(arr), tf.float64)\n",
    "        return x\n",
    "    \n",
    "    def probabilities(self, values):\n",
    "        val1 = tf.random_uniform(values.get_shape())\n",
    "        val1 = self.array_to_float32_tensor(val1)\n",
    "        val = k.backend.sigmoid(tf.sign(values - val1))\n",
    "        return val\n",
    "    \n",
    "    def forward_batch(self, visible):\n",
    "        a = self.array_to_float32_tensor(visible)\n",
    "        m = k.backend.dot(a, self.theta)\n",
    "        h = tf.add(m, self.hidden_bias)\n",
    "#         h = k.backend.relu(h, alpha=0.0, max_value=None)\n",
    "        #return k.backend.relu(h, alpha=0.0, max_value=None)\n",
    "        return k.backend.sigmoid(h)\n",
    "    \n",
    "    def forward_propagation(self, visible):\n",
    "        a=self.array_to_float32_tensor(visible)\n",
    "        b=self.array_to_float32_tensor(self.theta)\n",
    "        hb=self.array_to_float32_tensor(self.hidden_bias)\n",
    "        hidden = tf.add(k.backend.dot(a,b),hb)\n",
    "        #activate_hidden = k.backend.relu(hidden, alpha=0.0, max_value=None)\n",
    "        activate_hidden = k.backend.sigmoid(hidden)\n",
    "        return hidden, activate_hidden\n",
    "    \n",
    "    def backward_propagation(self, hidden): \n",
    "        a = self.array_to_float32_tensor(hidden)\n",
    "        b = self.array_to_float32_tensor(k.backend.transpose(self.theta))\n",
    "        dot=k.backend.dot(a, b)\n",
    "        visible = tf.add(dot,self.array_to_float32_tensor(self.visible_bias))\n",
    "        #activate_visible=k.backend.relu(visible, alpha=0.0, max_value=None)\n",
    "        activate_visible=k.backend.sigmoid(visible)\n",
    "        \n",
    "        return visible, activate_visible\n",
    "\n",
    "    def gibbs_sampling_h_given_v(self, sample_v):\n",
    "        l = 0.9\n",
    "        _, forward_prop = self.forward_propagation(sample_v)\n",
    "        sample_h = self.probabilities(forward_prop)\n",
    "        return sample_h,forward_prop\n",
    "    \n",
    "    \n",
    "    def gibbs_sampling_v_given_h(self, sample_h):\n",
    "        l = 0.9\n",
    "        _, backward_prop = self.backward_propagation(sample_h)\n",
    "        sample_v = self.probabilities(backward_prop)\n",
    "        return sample_v, backward_prop\n",
    "    \n",
    "    def gibbs_samplingh(self, sample_h):\n",
    "        sample_v, _ = self.gibbs_sampling_v_given_h(sample_h)\n",
    "        return self.gibbs_sampling_h_given_v(sample_v)\n",
    "\n",
    "    def gibbs_samplingv(self, sample_v):\n",
    "        sample_h, _ = self.gibbs_sampling_h_given_v(sample_v)\n",
    "        return self.gibbs_sampling_v_given_h(sample_h)  \n",
    "    \n",
    "    def visible_sampling(self, visible, n):\n",
    "        hidden_sample, _ = self.gibbs_sampling_h_given_v(visible)\n",
    "        visible_sample, _ = self.gibbs_sampling_v_given_h(hidden_sample)\n",
    "        for i in range(n-1):\n",
    "            hidden_sample, _ = self.gibbs_sampling_h_given_v(visible_sample)\n",
    "            visible_sample, _ = self.gibbs_sampling_v_given_h(hidden_sample)\n",
    "        return visible_sample, hidden_sample\n",
    "                                                \n",
    "    def Contrastive_Divergence(self, visible, alpha, n):\n",
    "        visible_sample, hidden_sample = self.visible_sampling(visible, n)\n",
    "        _, hidden = self.forward_propagation(visible)\n",
    "        self.hidden_bias += alpha * k.backend.mean(hidden - hidden_sample)\n",
    "        self.visible_bias += alpha * k.backend.mean(visible- visible_sample)\n",
    "        n = 1\n",
    "        h = tf.cast(k.backend.transpose(hidden), tf.float64)\n",
    "        visible = tf.cast(visible, tf.float64)\n",
    "        hidden_sample = tf.convert_to_tensor(hidden_sample)\n",
    "        visible_sample = tf.convert_to_tensor(visible_sample)\n",
    "        _ = visible.get_shape().as_list()[0]\n",
    "        a = tf.divide(tf.matmul(h, visible), _)\n",
    "        b  = tf.matmul(k.backend.transpose(hidden_sample), visible_sample)\n",
    "        c = tf.subtract(a,b)\n",
    "        self.theta += k.backend.transpose(alpha * c)\n",
    "        return  self.theta, self.visible_bias, self.hidden_bias\n",
    "                                       \n",
    "        \n",
    "    def energy_function(self, visible):\n",
    "        visible_sample, _ = self.visible_sampling(visible,1)\n",
    "        visible_term = k.backend.dot(visible_sample, k.backend.transpose(self.visible_bias))\n",
    "        weights_term = tf.add(k.backend.dot(visible_sample, self.theta), self.hidden_bias)\n",
    "        sum_term = k.backend.sum(k.backend.log(1 + k.backend.exp(weights_term)))\n",
    "        free_energy = -visible_term - sum_term\n",
    "        self.free_energy.append(k.backend.sum(free_energy))  \n",
    "        return free_energy\n",
    "    \n",
    "    def reconstruction(self, visible):\n",
    "        hidden, activate_hidden = self.forward_propagation(visible)\n",
    "        return self.backward_propagation(hidden)\n",
    "    \n",
    "    def reconstruction_error(self, data):\n",
    "        h = gibbs_samplingv(data)\n",
    "        v = gibbs_samplingh(h)\n",
    "        difference = tf.stop_gradient(data - v)\n",
    "        error = tf.reduce_sum(err * err)\n",
    "        return error\n",
    "\n",
    "    def training(self, visible, epochs=3, alpha=0.01):\n",
    "        epochs = epochs if epochs > 0 else 1\n",
    "        for i in range(epochs):\n",
    "            self.Contrastive_Divergence(visible, alpha, 1)\n",
    "        return\n",
    "    \n",
    "    # TODO try different costs\n",
    "    #             save_path = saver.save(sess, 'model/my_test_model', global_step=10, write_meta_graph=True)\n",
    "    #             self.cost = tf.sqrt(tf.reduce_mean(\n",
    "    #             tf.square(tf.subtract(self.inputs, self.reconstruction))))\n",
    "    #             tf.scalar_summary(\"train_loss\", self.cost)\n",
    "    #             self.summary = tf.merge_all_summaries()\n",
    "    \n",
    "    def show_energy(self):\n",
    "        plt.figure(figsize=(12,15))\n",
    "        plt.plot(self.free_energy)\n",
    "        plt.title('Free Energy')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Energy')\n",
    "        plt.plot()\n",
    "\n",
    "    def Something_about_Cost(self, visibles, n=1):\n",
    "        for i in range(n):\n",
    "            if i == 0:\n",
    "                visible_sample, pre_v = self.gibbs_samplingv(visibles)\n",
    "            else:\n",
    "                visible_sample, pre_v = self.gibbs_samplingv(visible_sample)\n",
    "        visible_sample = k.backend.stop_gradient(visible_sample)\n",
    "        recon_loss = -k.backend.mean(k.backend.sum(visibles * k.backend.log(k.backend.sigmoid(pre_v)) + k.backend.log(1 - k.backend.sigmoid(pre_v))))\n",
    "        contra_div_loss = k.backend.mean(self.energy_function(visibles)) - k.backend.mean(self.energy_function(visible_sample))\n",
    "        return recon_loss, contra_div_loss\n",
    "    \n",
    "    def check_overfitting(self, visible_train, visible_test):\n",
    "        ratio = k.backend.exp(-self.energy_function(visible_train) + self.energy_function(visible_test))\n",
    "        if r > 1:\n",
    "            print(\"overfitting\")\n",
    "            \n",
    "        else:\n",
    "            print(\"not overfitting\") \n",
    "                                                \n",
    "    def Pseudo_Likelihood(self, visibles):\n",
    "        visible_data_energy_function = self.energy_function(visibles)\n",
    "        print(visible_data_energy_function)\n",
    "        corrupted = visibles.copy()\n",
    "        index = (np.arange(visibles.shape[0]),  np.random.randint(0, visibles.shape[1], visibles.shape[0]))\n",
    "        corrupted[index] = 1 - corrupted[index]\n",
    "        visible_data_energy_function = self.energy_function(visibles)\n",
    "        print(visible_data_energy_function)\n",
    "        corrupted_energy_function = self.energy_function(corrupted)\n",
    "        print(corrupted_energy_function)\n",
    "        cost = k.backend.log(k.backend.sigmoid(corrupted_energy_function - visible_data_energy_function ))\n",
    "        print('cost', cost)\n",
    "        return cost\n",
    "\n",
    "    def show_energy(self):\n",
    "        plt.figure(figsize=(12,15))\n",
    "        plt.plot(self.free_energy)\n",
    "        plt.title('Free Energy')\n",
    "        plt.xlabel('epochs')\n",
    "        plt.ylabel('Energy')\n",
    "        plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DBN: # A DNB will consist of 2 RBMs\n",
    "    def __init__(self, visible):\n",
    "        # learning_rate = 0.01, n = 1\n",
    "        self.visible = visible\n",
    "\n",
    "    def train(self):\n",
    "        a = time.time()\n",
    "        rbm1 = RBM(visible_number=88200, hidden_number=50)\n",
    "        rbm2 = RBM(visible_number=50, hidden_number=2)\n",
    "        \n",
    "        num_epochs = 2\n",
    "        h = None\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            b = time.time()\n",
    "            print(\"%f minutes to init tensorflow\"%((b-a)/60.0))\n",
    "            a1 = time.time()\n",
    "            for epoch in range(num_epochs):\n",
    "                sess.run(rbm1.Contrastive_Divergence(self.visible, 0.01, 1))\n",
    "                np.save(\"theta_1.npy\", rbm1.theta.eval(session=sess))\n",
    "                np.save(\"hidden_bias_1.npy\", rbm1.hidden_bias.eval(session=sess))\n",
    "                np.save(\"visible_bias_1.npy\", rbm1.visible_bias.eval(session=sess))\n",
    "            \n",
    "            h = rbm1.forward_batch(self.visible)\n",
    "            h = h.eval(session=sess)\n",
    "            b1 = time.time()\n",
    "            print(\"%f minutes to train RBM 1\"%((b1-a1)/60.0))\n",
    "            \n",
    "            a2 = time.time()\n",
    "            for epoch in range(num_epochs):\n",
    "                sess.run(rbm2.Contrastive_Divergence(h, 0.01, 1))\n",
    "                np.save(\"theta_2.npy\", rbm2.theta.eval(session=sess))\n",
    "                np.save(\"hidden_bias_2.npy\", rbm2.hidden_bias.eval(session=sess))\n",
    "                np.save(\"visible_bias_2.npy\", rbm2.visible_bias.eval(session=sess))\n",
    "            b2 = time.time()\n",
    "            print(\"%f minutes to train RBM 2\"%((b2-a2)/60.0))\n",
    "            \n",
    "    def transform(self, data):\n",
    "        d = []\n",
    "        a = time.time()\n",
    "        t1 = np.load(\"theta_1.npy\")\n",
    "        h1 = np.load(\"hidden_bias_1.npy\")\n",
    "        v1 = np.load(\"visible_bias_1.npy\")\n",
    "        \n",
    "        t2 = np.load(\"theta_2.npy\")\n",
    "        h2 = np.load(\"hidden_bias_2.npy\")\n",
    "        v2 = np.load(\"visible_bias_2.npy\")\n",
    "        b = time.time()\n",
    "        print(\"%f minutes to load data\"%((b-a)/60.0))\n",
    "        \n",
    "        rbm1 = RBM(visible_number=88200, hidden_number=50, theta=t1, hidden_bias=h1, visible_bias=v1)\n",
    "        rbm2 = RBM(visible_number=50, hidden_number=2, theta=t2, hidden_bias=h2, visible_bias=v2)\n",
    "        \n",
    "        h = None\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            for i in range(len(data.shape[0])):\n",
    "                a1 = time.time()\n",
    "                h = rbm1.forward_propagation(data[i]).eval(session=sess)\n",
    "                h = rbm2.forward_propagation(h).eval(session=sess)\n",
    "                d.append(h)\n",
    "                b1 = time.time()\n",
    "            print(\"%f minutes to do forward propaganda\"%((b1-a1)/60.0))\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003500 minutes to init tensorflow\n"
     ]
    }
   ],
   "source": [
    "dbn = DBN(train_data)\n",
    "dbn.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002042 minutes to load data\n",
      "0.006072 minutes to do forward propaganda\n",
      "0.000295 minutes to load data\n",
      "0.003120 minutes to do forward propaganda\n",
      "0.000341 minutes to load data\n",
      "0.003240 minutes to do forward propaganda\n"
     ]
    }
   ],
   "source": [
    "train_h = dbn.transform(train_data)\n",
    "valid_h = dbn.transform(valid_data)\n",
    "test_h = dbn.transform(test_data)\n",
    "\n",
    "# print(train_h)\n",
    "# print(valid_h)\n",
    "#print(test_h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669],\n",
       "       [0.73171669, 0.73171669]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing some Binary Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 0.5\n",
      "Recall: 1.0\n",
      "None\n",
      "Accuracy: 0.5192307692307693\n",
      "Precision: 0.5192307692307693\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Support Vector Machines as classifier\n",
    "def SVM(data, label, test_data, test_labels):\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(data, label+0.0)\n",
    "    predictions = clf.predict(test_data)\n",
    "    print(\"Accuracy:\", metrics.accuracy_score(test_labels, predictions))\n",
    "    print(\"Precision:\",metrics.precision_score(test_labels, predictions))\n",
    "    print(\"Recall:\",metrics.recall_score(test_labels, predictions))\n",
    "\n",
    "print(SVM(train_h, train_labels, valid_h, valid_labels))\n",
    "SVM(train_h, train_labels, train_h, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: logistic regression\n",
    "def LR(data, labels):\n",
    "    clf = linear_model.LogisticRegression(C=1e5)\n",
    "    clf.fit(data, labels)\n",
    "    def model(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    loss = model(X_test * clf.coef_ + clf.intercept_).ravel()\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DO NOT RUN THE FOLLOWING CODE!!!\n",
    "\n",
    "The code takes the data that has the right labels which is the gender and puts it in a male and female folder. Reason for this is so that it can be easy to get the labels once I start classifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_data = np.genfromtxt('/files1b/856182/cv_corpus_v1/cv-valid-train.csv',dtype='unicode',delimiter=',')\n",
    "\n",
    "# good_files = []\n",
    "# female = []\n",
    "# male = []\n",
    "# num_files =  len(my_data)\n",
    "# for i in range(num_files):\n",
    "#     if my_data[i][5] == '':\n",
    "#         continue\n",
    "#     elif my_data[i][5] == 'female':\n",
    "#         female.append(my_data[i])\n",
    "#     elif my_data[i][5] == 'male':\n",
    "#         male.append(my_data[i]) \n",
    "\n",
    "# for i in range(len(male)):\n",
    "#     shutil.copy('/files1b/856182/cv_corpus_v1/'+male[i][0],'/files1b/856182/cv_corpus_v1/datasets/male/')\n",
    "#     print (\"File number \"+str(i)+\" out of \"+str(len(male)))\n",
    "    \n",
    "# for i in range(len(female)):\n",
    "#     shutil.copy('/files1b/856182/cv_corpus_v1/'+female[i][0],'/files1b/856182/cv_corpus_v1/datasets/female/')\n",
    "#     print (\"File number \"+str(i)+\" out of \"+str(len(female)))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
